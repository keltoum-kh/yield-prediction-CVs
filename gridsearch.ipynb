{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00604825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import box\n",
    "from fiona.crs import from_epsg\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, StratifiedKFold,GroupKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "import xlsxwriter\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn import ensemble\n",
    "from rasterio.plot import show\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio import merge\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, StratifiedKFold,GroupKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBRegressor\n",
    "import xlsxwriter\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn import ensemble\n",
    "from rasterio.plot import show\n",
    "import glob\n",
    "import os\n",
    "import rasterio\n",
    "from rasterio import merge\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVR\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "712a8cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single grid run (cell_size = 0.005° ≈ 500 m)\n",
      "R²   : 0.3068\n",
      "MAE  : 816.94\n",
      "RMSE : 1085.44\n"
     ]
    }
   ],
   "source": [
    "# --- imports ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from sklearn.model_selection import GroupKFold, cross_val_predict\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "\n",
    "# --- data ---\n",
    "workbook = 'field_point.xlsx'\n",
    "sheet = 'all'\n",
    "df = pd.read_excel(workbook, sheet)\n",
    "\n",
    "# features/target\n",
    "X = df.iloc[:, 5:-1].values   # all cols except last (from col 5)\n",
    "Y = df.iloc[:, -1].values     # last col as target\n",
    "\n",
    "# ---------- ONE GRID ONLY (≈500 m) ----------\n",
    "cell_size = 0.005  # degrees (~500 m). This is the FIRST value in your old loop.\n",
    "\n",
    "# points -> GeoDataFrame (WGS84)\n",
    "geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# save & reload (kept your behavior)\n",
    "gdf.to_file(\"SA.shp\")\n",
    "SA = gpd.read_file(\"SA.shp\")\n",
    "\n",
    "# build the grid (in degrees, like your original)\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "rows = int((maxy - miny) // cell_size + 3)\n",
    "cols = int((maxx - minx) // cell_size + 3)\n",
    "\n",
    "polygons = []\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        xmin = minx - 1.5 * cell_size + j * cell_size\n",
    "        xmax = xmin + cell_size\n",
    "        ymin = miny - 1.5 * cell_size + i * cell_size\n",
    "        ymax = ymin + cell_size\n",
    "        polygons.append(Polygon([(xmin, ymin), (xmin, ymax), (xmax, ymax), (xmax, ymin)]))\n",
    "\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': polygons}, crs=\"EPSG:4326\")\n",
    "# add a real FID column so later lookups work\n",
    "grid_gdf['FID'] = np.arange(len(grid_gdf), dtype=int)\n",
    "\n",
    "grid_shapefile = f\"grid_{cell_size}.shp\"\n",
    "grid_gdf.to_file(grid_shapefile)\n",
    "\n",
    "# load grid\n",
    "grid = gpd.read_file(grid_shapefile)\n",
    "\n",
    "# assign each point to a grid cell (within -> nearest fallback)\n",
    "grid_df = pd.DataFrame(columns=[f'grid_{cell_size}'])\n",
    "for idx, point in SA.iterrows():\n",
    "    containing = grid[grid.contains(point.geometry)]\n",
    "    if not containing.empty:\n",
    "        field_value = int(containing.iloc[0]['FID'])\n",
    "        grid_df.loc[idx] = [field_value]\n",
    "    else:\n",
    "        nearest = grid.loc[grid.distance(point.geometry).idxmin()]\n",
    "        field_value = int(nearest['FID'])\n",
    "        grid_df.loc[idx] = [field_value]\n",
    "\n",
    "# GroupKFold on that single grid\n",
    "block = grid_df[f'grid_{cell_size}'].astype(int).values\n",
    "group_kfold = GroupKFold(n_splits=3)\n",
    "block_kfold = group_kfold.split(X, Y, block)\n",
    "train_indices, test_indices = [list(traintest) for traintest in zip(*block_kfold)]\n",
    "block_cv = [*zip(train_indices, test_indices)]\n",
    "\n",
    "# model + CV prediction\n",
    "model = xgb.XGBRegressor()\n",
    "Y_pred = cross_val_predict(model, X, Y, cv=block_cv)\n",
    "\n",
    "# metrics\n",
    "r2 = metrics.r2_score(Y, Y_pred)\n",
    "mae = float(metrics.mean_absolute_error(Y, Y_pred))\n",
    "rmse = float(np.sqrt(metrics.mean_squared_error(Y, Y_pred)))\n",
    "\n",
    "print(\"Single grid run (cell_size = 0.005° ≈ 500 m)\")\n",
    "print(f\"R²   : {r2:.4f}\")\n",
    "print(f\"MAE  : {mae:.2f}\")\n",
    "print(f\"RMSE : {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ff0211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single grid run (cell_size = 0.005° ≈ 500 m) — mean of 3 folds\n",
      "R²   : 0.3016\n",
      "MAE  : 816.94\n",
      "RMSE : 1084.79\n"
     ]
    }
   ],
   "source": [
    "\n",
    "workbook = 'field_point.xlsx'\n",
    "sheet = 'all'\n",
    "df = pd.read_excel(workbook, sheet)\n",
    "\n",
    "# features/target\n",
    "X = df.iloc[:, 5:-1].values   # all cols except last (from col 5)\n",
    "Y = df.iloc[:, -1].values     # last col as target\n",
    "\n",
    "# ---------- ONE GRID ONLY (≈500 m) ----------\n",
    "cell_size = 0.005  # degrees (~500 m). This is the first value in your old loop.\n",
    "\n",
    "# points -> GeoDataFrame (WGS84)\n",
    "geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# save & reload (kept your behavior)\n",
    "gdf.to_file(\"SA.shp\")\n",
    "SA = gpd.read_file(\"SA.shp\")\n",
    "\n",
    "# build the grid (in degrees, like your original)\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "rows = int((maxy - miny) // cell_size + 3)\n",
    "cols = int((maxx - minx) // cell_size + 3)\n",
    "\n",
    "polygons = []\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        xmin = minx - 1.5 * cell_size + j * cell_size\n",
    "        xmax = xmin + cell_size\n",
    "        ymin = miny - 1.5 * cell_size + i * cell_size\n",
    "        ymax = ymin + cell_size\n",
    "        polygons.append(Polygon([(xmin, ymin), (xmin, ymax), (xmax, ymax), (xmax, ymin)]))\n",
    "\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': polygons}, crs=\"EPSG:4326\")\n",
    "# add a real FID column so later lookups work\n",
    "grid_gdf['FID'] = np.arange(len(grid_gdf), dtype=int)\n",
    "\n",
    "grid_shapefile = f\"grid_{cell_size}.shp\"\n",
    "grid_gdf.to_file(grid_shapefile)\n",
    "\n",
    "# load grid\n",
    "grid = gpd.read_file(grid_shapefile)\n",
    "\n",
    "# assign each point to a grid cell (within -> nearest fallback)\n",
    "grid_df = pd.DataFrame(columns=[f'grid_{cell_size}'])\n",
    "for idx, point in SA.iterrows():\n",
    "    containing = grid[grid.contains(point.geometry)]\n",
    "    if not containing.empty:\n",
    "        field_value = int(containing.iloc[0]['FID'])\n",
    "        grid_df.loc[idx] = [field_value]\n",
    "    else:\n",
    "        # nearest fallback\n",
    "        nearest = grid.loc[grid.distance(point.geometry).idxmin()]\n",
    "        field_value = int(nearest['FID'])\n",
    "        grid_df.loc[idx] = [field_value]\n",
    "\n",
    "# GroupKFold on that single grid\n",
    "block = grid_df[f'grid_{cell_size}'].astype(int).values\n",
    "group_kfold = GroupKFold(n_splits=3)\n",
    "block_kfold = list(group_kfold.split(X, Y, block))\n",
    "\n",
    "# ---- Train per fold and average metrics ----\n",
    "r2s, maes, rmses = [], [], []\n",
    "for train_idx, test_idx in block_kfold:\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(X[train_idx], Y[train_idx])\n",
    "    y_pred = model.predict(X[test_idx])\n",
    "    y_true = Y[test_idx]\n",
    "\n",
    "    r2s.append(metrics.r2_score(y_true, y_pred))\n",
    "    maes.append(metrics.mean_absolute_error(y_true, y_pred))\n",
    "    rmses.append(np.sqrt(metrics.mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# mean scores across the 3 folds\n",
    "r2_mean = float(np.mean(r2s))\n",
    "mae_mean = float(np.mean(maes))\n",
    "rmse_mean = float(np.mean(rmses))\n",
    "\n",
    "print(\"Single grid run (cell_size = 0.005° ≈ 500 m) — mean of 3 folds\")\n",
    "print(f\"R²   : {r2_mean:.4f}\")\n",
    "print(f\"MAE  : {mae_mean:.2f}\")\n",
    "print(f\"RMSE : {rmse_mean:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "807927cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1c194ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-4318ca8d3915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m          \u001b[1;31m# stable; change to -1 if you want parallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         )\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1049\u001b[0m                 \u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m             )\n\u001b[1;32m-> 1051\u001b[1;33m             self._Booster = train(\n\u001b[0m\u001b[0;32m   1052\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1919\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "workbook = 'field_point.xlsx'\n",
    "sheet = 'all'\n",
    "df = pd.read_excel(workbook, sheet)\n",
    "\n",
    "# features/target\n",
    "X = df.iloc[:, 5:-1].values   # all cols except last (from col 5)\n",
    "Y = df.iloc[:, -1].values     # last col as target\n",
    "\n",
    "# ---------- ONE GRID ONLY (≈500 m) ----------\n",
    "cell_size = 0.005  # degrees (~500 m). This is the first value in your old loop.\n",
    "\n",
    "# points -> GeoDataFrame (WGS84)\n",
    "geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "# save & reload (kept your behavior)\n",
    "gdf.to_file(\"SA.shp\")\n",
    "SA = gpd.read_file(\"SA.shp\")\n",
    "\n",
    "# build the grid (in degrees, like your original)\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "rows = int((maxy - miny) // cell_size + 3)\n",
    "cols = int((maxx - minx) // cell_size + 3)\n",
    "\n",
    "polygons = []\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        xmin = minx - 1.5 * cell_size + j * cell_size\n",
    "        xmax = xmin + cell_size\n",
    "        ymin = miny - 1.5 * cell_size + i * cell_size\n",
    "        ymax = ymin + cell_size\n",
    "        polygons.append(Polygon([(xmin, ymin), (xmin, ymax), (xmax, ymax), (xmax, ymin)]))\n",
    "\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': polygons}, crs=\"EPSG:4326\")\n",
    "# add a real FID column so later lookups work\n",
    "grid_gdf['FID'] = np.arange(len(grid_gdf), dtype=int)\n",
    "\n",
    "grid_shapefile = f\"grid_{cell_size}.shp\"\n",
    "grid_gdf.to_file(grid_shapefile)\n",
    "\n",
    "# load grid\n",
    "grid = gpd.read_file(grid_shapefile)\n",
    "\n",
    "# assign each point to a grid cell (within -> nearest fallback)\n",
    "grid_df = pd.DataFrame(columns=[f'grid_{cell_size}'])\n",
    "for idx, point in SA.iterrows():\n",
    "    containing = grid[grid.contains(point.geometry)]\n",
    "    if not containing.empty:\n",
    "        field_value = int(containing.iloc[0]['FID'])\n",
    "        grid_df.loc[idx] = [field_value]\n",
    "    else:\n",
    "        # nearest fallback (distance in degrees; matches your original approach)\n",
    "        nearest = grid.loc[grid.distance(point.geometry).idxmin()]\n",
    "        field_value = int(nearest['FID'])\n",
    "        grid_df.loc[idx] = [field_value]\n",
    "\n",
    "# GroupKFold on that single grid\n",
    "block = grid_df[f'grid_{cell_size}'].astype(int).values\n",
    "n_groups = pd.Series(block).nunique()\n",
    "if n_groups < 2:\n",
    "    raise ValueError(f\"Not enough unique grid blocks for CV (found {n_groups}).\")\n",
    "n_splits = 3 if n_groups >= 3 else 2\n",
    "group_kfold = GroupKFold(n_splits=n_splits)\n",
    "block_kfold = list(group_kfold.split(X, Y, block))\n",
    "\n",
    "# ---------- Hyperparameter grid (Zhang et al., 2022 ranges) ----------\n",
    "max_depth_grid      = list(range(1, 6))              # 1..5\n",
    "n_estimators_grid   = list(range(100, 1001, 100))    # 100..1000 step 100\n",
    "learning_rate_grid  = [0.1, 0.2, 0.3]               # 0.1..0.3 step 0.1\n",
    "colsample_grid      = [i/10 for i in range(1, 10)]  # 0.1..0.9 step 0.1\n",
    "\n",
    "# ---------- Grid search loop ----------\n",
    "results = []\n",
    "for md, ne, lr, cs in product(max_depth_grid, n_estimators_grid, learning_rate_grid, colsample_grid):\n",
    "    r2s, maes, rmses = [], [], []\n",
    "    for train_idx, test_idx in block_kfold:\n",
    "        model = xgb.XGBRegressor(\n",
    "            max_depth=md,\n",
    "            n_estimators=ne,\n",
    "            learning_rate=lr,\n",
    "            colsample_bytree=cs,\n",
    "            random_state=42,  # for repeatability\n",
    "            n_jobs=1          # stable; change to -1 if you want parallel\n",
    "        )\n",
    "        model.fit(X[train_idx], Y[train_idx])\n",
    "        y_pred = model.predict(X[test_idx])\n",
    "        y_true = Y[test_idx]\n",
    "\n",
    "        r2s.append(metrics.r2_score(y_true, y_pred))\n",
    "        maes.append(metrics.mean_absolute_error(y_true, y_pred))\n",
    "        rmses.append(np.sqrt(metrics.mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "    results.append({\n",
    "        'max_depth': md,\n",
    "        'n_estimators': ne,\n",
    "        'learning_rate': lr,\n",
    "        'colsample_bytree': cs,\n",
    "        'R2_mean': float(np.mean(r2s)),\n",
    "        'MAE_mean': float(np.mean(maes)),\n",
    "        'RMSE_mean': float(np.mean(rmses))\n",
    "    })\n",
    "\n",
    "# ---------- Select the best combo by highest mean R² (tie-breakers: lowest RMSE, then lowest MAE) ----------\n",
    "best = max(\n",
    "    results,\n",
    "    key=lambda d: (d['R2_mean'], -d['RMSE_mean'], -d['MAE_mean'])\n",
    ")\n",
    "\n",
    "# ---------- Print only the best ----------\n",
    "print(\"Best hyperparameters (by mean R², with RMSE/MAE tie-breakers):\")\n",
    "print(f\"  max_depth       = {best['max_depth']}\")\n",
    "print(f\"  n_estimators    = {best['n_estimators']}\")\n",
    "print(f\"  learning_rate   = {best['learning_rate']}\")\n",
    "print(f\"  colsample_bytree= {best['colsample_bytree']}\")\n",
    "print(\"\\nMean CV scores (3 folds):\")\n",
    "print(f\"  R²   : {best['R2_mean']:.4f}\")\n",
    "print(f\"  RMSE : {best['RMSE_mean']:.2f}\")\n",
    "print(f\"  MAE  : {best['MAE_mean']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7e13e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "workbook = 'field_point.xlsx'\n",
    "sheet = 'all'\n",
    "df = pd.read_excel(workbook, sheet)\n",
    "\n",
    "X = df.iloc[:, 5:-1].values  \n",
    "Y = df.iloc[:, -1].values   \n",
    "\n",
    "\n",
    "cell_size = 0.005  \n",
    "\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n",
    "\n",
    "\n",
    "gdf.to_file(\"SA.shp\")\n",
    "SA = gpd.read_file(\"SA.shp\")\n",
    "\n",
    "\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "rows = int((maxy - miny) // cell_size + 3)\n",
    "cols = int((maxx - minx) // cell_size + 3)\n",
    "\n",
    "polygons = []\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        xmin = minx - 1.5 * cell_size + j * cell_size\n",
    "        xmax = xmin + cell_size\n",
    "        ymin = miny - 1.5 * cell_size + i * cell_size\n",
    "        ymax = ymin + cell_size\n",
    "        polygons.append(Polygon([(xmin, ymin), (xmin, ymax), (xmax, ymax), (xmax, ymin)]))\n",
    "\n",
    "grid_gdf = gpd.GeoDataFrame({'geometry': polygons}, crs=\"EPSG:4326\")\n",
    "# add a real FID column so later lookups work\n",
    "grid_gdf['FID'] = np.arange(len(grid_gdf), dtype=int)\n",
    "\n",
    "grid_shapefile = f\"grid_{cell_size}.shp\"\n",
    "grid_gdf.to_file(grid_shapefile)\n",
    "\n",
    "# load grid\n",
    "grid = gpd.read_file(grid_shapefile)\n",
    "\n",
    "# assign each point to a grid cell (within -> nearest fallback)\n",
    "grid_df = pd.DataFrame(columns=[f'grid_{cell_size}'])\n",
    "for idx, point in SA.iterrows():\n",
    "    containing = grid[grid.contains(point.geometry)]\n",
    "    if not containing.empty:\n",
    "        field_value = int(containing.iloc[0]['FID'])\n",
    "        grid_df.loc[idx] = [field_value]\n",
    "    else:\n",
    "\n",
    "        nearest = grid.loc[grid.distance(point.geometry).idxmin()]\n",
    "        field_value = int(nearest['FID'])\n",
    "        grid_df.loc[idx] = [field_value]\n",
    "\n",
    "\n",
    "block = grid_df[f'grid_{cell_size}'].astype(int).values\n",
    "n_groups = pd.Series(block).nunique()\n",
    "if n_groups < 2:\n",
    "    raise ValueError(f\"Not enough unique grid blocks for CV (found {n_groups}).\")\n",
    "n_splits = 3 if n_groups >= 3 else 2\n",
    "group_kfold = GroupKFold(n_splits=n_splits)\n",
    "block_kfold = list(group_kfold.split(X, Y, block))\n",
    "\n",
    "\n",
    "max_depth_grid      = list(range(1, 6))              \n",
    "n_estimators_grid   = list(range(100, 1001, 100))    \n",
    "learning_rate_grid  = [0.1, 0.2, 0.3]               \n",
    "colsample_grid      = [i/10 for i in range(1, 10)] \n",
    "\n",
    "\n",
    "results = []\n",
    "for md, ne, lr, cs in product(max_depth_grid, n_estimators_grid, learning_rate_grid, colsample_grid):\n",
    "    r2s, maes, rmses = [], [], []\n",
    "    for train_idx, test_idx in block_kfold:\n",
    "        model = xgb.XGBRegressor(\n",
    "            max_depth=md,\n",
    "            n_estimators=ne,\n",
    "            learning_rate=lr,\n",
    "            colsample_bytree=cs,\n",
    "            random_state=42,  \n",
    "            n_jobs=1          \n",
    "        )\n",
    "        model.fit(X[train_idx], Y[train_idx])\n",
    "        y_pred = model.predict(X[test_idx])\n",
    "        y_true = Y[test_idx]\n",
    "\n",
    "        r2s.append(metrics.r2_score(y_true, y_pred))\n",
    "        maes.append(metrics.mean_absolute_error(y_true, y_pred))\n",
    "        rmses.append(np.sqrt(metrics.mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "    results.append({\n",
    "        'max_depth': md,\n",
    "        'n_estimators': ne,\n",
    "        'learning_rate': lr,\n",
    "        'colsample_bytree': cs,\n",
    "        'R2_mean': float(np.mean(r2s)),\n",
    "        'MAE_mean': float(np.mean(maes)),\n",
    "        'RMSE_mean': float(np.mean(rmses))\n",
    "    })\n",
    "\n",
    "\n",
    "best = max(\n",
    "    results,\n",
    "    key=lambda d: (d['R2_mean'], -d['RMSE_mean'], -d['MAE_mean'])\n",
    ")\n",
    "\n",
    "print(\"Best hyperparameters (by mean R², with RMSE/MAE tie-breakers):\")\n",
    "print(f\"  max_depth       = {best['max_depth']}\")\n",
    "print(f\"  n_estimators    = {best['n_estimators']}\")\n",
    "print(f\"  learning_rate   = {best['learning_rate']}\")\n",
    "print(f\"  colsample_bytree= {best['colsample_bytree']}\")\n",
    "print(\"\\nMean CV scores (3 folds):\")\n",
    "print(f\"  R²   : {best['R2_mean']:.4f}\")\n",
    "print(f\"  RMSE : {best['RMSE_mean']:.2f}\")\n",
    "print(f\"  MAE  : {best['MAE_mean']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
